Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=1000, mem_mib=954, disk_mb=1000, disk_mib=954, mem_gb=16, time_min=119
Select jobs to execute...

[Fri Jul 19 13:16:10 2024]
rule fast_condor:
    input: condor_inputs/RA17_22/character_bin_matrix.csv, condor_inputs/RA17_22/alt_readcounts.csv, condor_inputs/RA17_22/total_readcounts.csv, condor_inputs/RA17_22/germline_mutations.txt, condor_inputs/RA17_22/somatic_mutations.txt, /data/iacobuzc/haochen/Tapestri_main_manuscript_analysis/data_compiled/falcon_solutions/RA17_22.unique_cn_clone_profiles.csv
    output: condor_outputs/RA17_22/out_tree.newick, condor_outputs/pickle_files/RA17_22_self.solT_cell
    log: condor_outputs/RA17_22/condor_outputs.log, condor_outputs/RA17_22/condor_outputs.err.log
    jobid: 0
    reason: Missing output files: condor_outputs/RA17_22/out_tree.newick, condor_outputs/pickle_files/RA17_22_self.solT_cell
    wildcards: patient=RA17_22
    threads: 4
    resources: mem_mb=1000, mem_mib=954, disk_mb=1000, disk_mib=954, tmpdir=/scratch/lsftmp/8217025.tmpdir, mem_gb=16, time_min=119

Activating conda environment: condor
[Fri Jul 19 13:16:58 2024]
Finished job 0.
1 of 1 steps (100%) done
